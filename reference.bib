
@article{wei2025deepseek,
  title   = {DeepSeek-OCR: Contexts Optical Compression},
  author  = {Wei, Haoran and Sun, Yaofeng and Li, Yukun},
  journal = {arXiv preprint arXiv:2510.18234},
  year    = {2025}
}
@article{wei2026deepseek,
  title   = {DeepSeek-OCR 2: Visual Causal Flow},
  author  = {Wei, Haoran and Sun, Yaofeng and Li, Yukun},
  journal = {arXiv preprint arXiv:2601.20552},
  year    = {2026}
}


@inproceedings{liu2025pointsreader,
  author    = {Yuan Liu and Z. Zhao and L. Tian and others},
  title     = {POINTS-Reader: Distillation-Free Adaptation of Vision-Language Models for Document Conversion},
  booktitle = {Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing},
  year      = {2025},
  pages     = {1576--1601},
  month     = {November}
}
@article{wang2025internvl35,
  author  = {Weiyun Wang and Z. Gao and L. Gu and others},
  title   = {InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency},
  journal = {arXiv preprint arXiv:2508.18265},
  year    = {2025}
}
@article{team2023gemini,
  title   = {Gemini: a family of highly capable multimodal models},
  author  = {Team, Gemini and Anil, Rohan and Borgeaud, Sebastian and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M and Hauth, Anja and Millican, Katie and others},
  journal = {arXiv preprint arXiv:2312.11805},
  year    = {2023}
}

@misc{seedseed1,
  title  = {Seed1. 8 Model Card: Towards Generalized Real-World Agency},
  author = {Seed, Bytedance},
  year   = {2025}
}
@article{bai2025qwen3vltechnicalreport,
  author  = {S. Bai and Y. Cai and R. Chen and others},
  title   = {Qwen3-VL Technical Report},
  journal = {arXiv preprint arXiv:2511.21631},
  year    = {2025},
  url     = {https://arxiv.org/abs/2511.21631}
}
@article{liu2025deepseek,
  title   = {Deepseek-v3. 2: Pushing the frontier of open large language models},
  author  = {Liu, Aixin and Mei, Aoxue and Lin, Bangcai and Xue, Bing and Wang, Bingxuan and Xu, Bingzheng and Wu, Bochao and Zhang, Bowei and Lin, Chaofan and Dong, Chen and others},
  journal = {arXiv preprint arXiv:2512.02556},
  year    = {2025}
}

@article{deepseekv2,
  title   = {Deepseek-v2: A strong, economical, and efficient mixture-of-experts language model},
  author  = {Liu, Aixin and Feng, Bei and Wang, Bin and Wang, Bingxuan and Liu, Bo and Zhao, Chenggang and Dengr, Chengqi and Ruan, Chong and Dai, Damai and Guo, Daya and others},
  journal = {arXiv preprint arXiv:2405.04434},
  year    = {2024}
}

@article{su2021roformer,
  title   = {RoFormer: Enhanced Transformer with Rotary Position Embedding},
  author  = {Su, Jianlin and Lu, Yu and Pan, Shaojie and Wen, Bo and Liu, Yunfeng},
  journal = {arXiv preprint arXiv:2104.09864},
  year    = {2021}
}

@article{liu2025context,
  title   = {Context cascade compression: Exploring the upper limits of text compression},
  author  = {Liu, Fanfan and Qiu, Haibo},
  journal = {arXiv preprint arXiv:2511.15244},
  year    = {2025}
}


@inproceedings{ren2015faster,
  title     = {Faster r-cnn: Towards real-time object detection with region proposal networks},
  author    = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
  booktitle = {Advances in neural information processing systems},
  pages     = {91--99},
  year      = {2015}
}
@inproceedings{redmon2017yolo9000,
  title     = {YOLO9000: better, faster, stronger},
  author    = {Redmon, Joseph and Farhadi, Ali},
  booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages     = {7263--7271},
  year      = {2017}
}
@inproceedings{lin2017feature,
  title     = {Feature pyramid networks for object detection},
  author    = {Lin, Tsung-Yi and Doll{\'a}r, Piotr and Girshick, Ross and He, Kaiming and Hariharan, Bharath and Belongie, Serge},
  booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages     = {2117--2125},
  year      = {2017}
}
@inproceedings{devlin2019bert,
  title     = {Bert: Pre-training of deep bidirectional transformers for language understanding},
  author    = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  booktitle = {Proceedings of the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies, volume 1 (long and short papers)},
  pages     = {4171--4186},
  year      = {2019}
}

@article{dong2023dreamllm,
  title   = {Dreamllm: Synergistic multimodal comprehension and creation},
  author  = {Dong, Runpei and Han, Chunrui and Peng, Yuang and Qi, Zekun and Ge, Zheng and Yang, Jinrong and Zhao, Liang and Sun, Jianjian and Zhou, Hongyu and Wei, Haoran and others},
  journal = {arXiv preprint arXiv:2309.11499},
  year    = {2023}
}

@inproceedings{carion2020end,
  title        = {End-to-end object detection with transformers},
  author       = {Carion, Nicolas and Massa, Francisco and Synnaeve, Gabriel and Usunier, Nicolas and Kirillov, Alexander and Zagoruyko, Sergey},
  booktitle    = {European conference on computer vision},
  pages        = {213--229},
  year         = {2020},
  organization = {Springer}
}


@inproceedings{li2023blip,
  title        = {Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models},
  author       = {Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven},
  booktitle    = {International conference on machine learning},
  pages        = {19730--19742},
  year         = {2023},
  organization = {PMLR}
}

@article{dosovitskiy2020image,
  title   = {An image is worth 16x16 words: Transformers for image recognition at scale},
  author  = {Dosovitskiy, Alexey},
  journal = {arXiv preprint arXiv:2010.11929},
  year    = {2020}
}

@article{liu2024focus_fox,
  title   = {Focus Anywhere for Fine-grained Multi-page Document Understanding},
  author  = {Liu, Chenglong and Wei, Haoran and Chen, Jinyue and Kong, Lingyu and Ge, Zheng and Zhu, Zining and Zhao, Liang and Sun, Jianjian and Han, Chunrui and Zhang, Xiangyu},
  journal = {arXiv preprint arXiv:2405.14295},
  year    = {2024}
}

@inproceedings{ouyang2025omnidocbench,
  title     = {Omnidocbench: Benchmarking diverse pdf document parsing with comprehensive annotations},
  author    = {Ouyang, Linke and Qu, Yuan and Zhou, Hongbin and Zhu, Jiawei and Zhang, Rui and Lin, Qunshu and Wang, Bin and Zhao, Zhiyuan and Jiang, Man and Zhao, Xiaomeng and others},
  booktitle = {Proceedings of the Computer Vision and Pattern Recognition Conference},
  pages     = {24838--24848},
  year      = {2025}
}

@inproceedings{goyal2017making,
  title     = {Making the v in vqa matter: Elevating the role of image understanding in visual question answering},
  author    = {Goyal, Yash and Khot, Tejas and Summers-Stay, Douglas and Batra, Dhruv and Parikh, Devi},
  booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages     = {6904--6913},
  year      = {2017}
}

@article{masry2022chartqa,
  title   = {ChartQA: A benchmark for question answering about charts with visual and logical reasoning},
  author  = {Masry, Ahmed and Long, Do Xuan and Tan, Jia Qing and Joty, Shafiq and Hoque, Enamul},
  journal = {arXiv preprint arXiv:2203.10244},
  year    = {2022}
}
@inproceedings{TextVQA,
  title     = {Towards vqa models that can read},
  author    = {Singh, Amanpreet and Natarajan, Vivek and Shah, Meet and Jiang, Yu and Chen, Xinlei and Batra, Dhruv and Parikh, Devi and Rohrbach, Marcus},
  booktitle = {Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages     = {8317--8326},
  year      = {2019}
}
@article{wang2025step,
  title   = {Step-3 is Large yet Affordable: Model-system Co-design for Cost-effective Decoding},
  author  = {Wang, Bin and Wang, Bojun and Wan, Changyi and Huang, Guanzhe and Hu, Hanpeng and Jia, Haonan and Nie, Hao and Li, Mingliang and Chen, Nuo and Chen, Siyu and others},
  journal = {arXiv preprint arXiv:2507.19427},
  year    = {2025}
}
@inproceedings{wei2024vary,
  title        = {Vary: Scaling up the vision vocabulary for large vision-language model},
  author       = {Wei, Haoran and Kong, Lingyu and Chen, Jinyue and Zhao, Liang and Ge, Zheng and Yang, Jinrong and Sun, Jianjian and Han, Chunrui and Zhang, Xiangyu},
  booktitle    = {European Conference on Computer Vision},
  pages        = {408--424},
  year         = {2024},
  organization = {Springer}
}
@article{Qwen-VL,
  title   = {Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond},
  author  = {Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren},
  journal = {arXiv preprint arXiv:2308.12966},
  year    = {2023}
}

@article{yu2023mm,
  title   = {Mm-vet: Evaluating large multimodal models for integrated capabilities},
  author  = {Yu, Weihao and Yang, Zhengyuan and Li, Linjie and Wang, Jianfeng and Lin, Kevin and Liu, Zicheng and Wang, Xinchao and Wang, Lijuan},
  journal = {arXiv preprint arXiv:2308.02490},
  year    = {2023}
}
@inproceedings{kazemzadeh2014referitgame,
  title     = {Referitgame: Referring to objects in photographs of natural scenes},
  author    = {Kazemzadeh, Sahar and Ordonez, Vicente and Matten, Mark and Berg, Tamara},
  booktitle = {Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)},
  pages     = {787--798},
  year      = {2014}
}

@article{kirillov2023segment,
  title   = {Segment anything},
  author  = {Kirillov, Alexander and Mintun, Eric and Ravi, Nikhila and Mao, Hanzi and Rolland, Chloe and Gustafson, Laura and Xiao, Tete and Whitehead, Spencer and Berg, Alexander C and Lo, Wan-Yen and others},
  journal = {arXiv preprint arXiv:2304.02643},
  year    = {2023}
}
@inproceedings{radford2021learning,
  title        = {Learning transferable visual models from natural language supervision},
  author       = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle    = {International conference on machine learning},
  pages        = {8748--8763},
  year         = {2021},
  organization = {PMLR}
}
@article{wang2024qwen2,
  title   = {Qwen2-vl: Enhancing vision-language model's perception of the world at any resolution},
  author  = {Wang, Peng and Bai, Shuai and Tan, Sinan and Wang, Shijie and Fan, Zhihao and Bai, Jinze and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and others},
  journal = {arXiv preprint arXiv:2409.12191},
  year    = {2024}
}
@article{chen2024internvl2,
  title   = {How Far Are We to GPT-4V? Closing the Gap to Commercial Multimodal Models with Open-Source Suites},
  author  = {Chen, Zhe and Wang, Weiyun and Tian, Hao and Ye, Shenglong and Gao, Zhangwei and Cui, Erfei and Tong, Wenwen and Hu, Kongzhi and Luo, Jiapeng and Ma, Zheng and others},
  journal = {arXiv preprint arXiv:2404.16821},
  year    = {2024}
}

@article{dehghani2023patch,
  title   = {Patch n' Pack: NaViT, a Vision Transformer for any Aspect Ratio and Resolution},
  author  = {Dehghani, Mostafa and Djolonga, Josip and Mustafa, Basil and Padlewski, Piotr and Heek, Jonathan and Gilmer, Justin and Steiner, Andreas and Caron, Mathilde and Geirhos, Robert and Alabdulmohsin, Ibrahim and others},
  journal = {Advances in Neural Information Processing Systems},
  volume  = {36},
  pages   = {3632--3656},
  year    = {2023}
}
@article{blecher2023nougat,
  title   = {Nougat: Neural optical understanding for academic documents},
  author  = {Blecher, Lukas and Cucurull, Guillem and Scialom, Thomas and Stojnic, Robert},
  journal = {arXiv preprint arXiv:2308.13418},
  year    = {2023}
}
@article{wei2024general,
  title   = {General ocr theory: Towards ocr-2.0 via a unified end-to-end model},
  author  = {Wei, Haoran and Liu, Chenglong and Chen, Jinyue and Wang, Jia and Kong, Lingyu and Xu, Yanming and Ge, Zheng and Zhao, Liang and Sun, Jianjian and Peng, Yuang and others},
  journal = {arXiv preprint arXiv:2409.01704},
  year    = {2024}
}

@article{deepseek32,
  title   = {Deepseek-v3. 2: Pushing the frontier of open large language models},
  author  = {Liu, Aixin and Mei, Aoxue and Lin, Bangcai and Xue, Bing and Wang, Bingxuan and Xu, Bingzheng and Wu, Bochao and Zhang, Bowei and Lin, Chaofan and Dong, Chen and others},
  journal = {arXiv preprint arXiv:2512.02556},
  year    = {2025}
}
@article{pang2023fozen,
  title   = {Frozen transformers in language models are effective visual encoder layers},
  author  = {Pang, Ziqi and Xie, Ziyang and Man, Yunze and Wang, Yu-Xiong},
  journal = {arXiv preprint arXiv:2310.12973},
  year    = {2023}
}
@misc{fuyu8b_model,
  author       = {Adept},
  title        = {Fuyu-8B},
  year         = {2023},
  publisher    = {Hugging Face},
  howpublished = {\url{https://huggingface.co/adept/fuyu-8b}}
}
@article{wang2023neural,
  title   = {Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers},
  author  = {Wang, Chengyi and Chen, Sanyuan and Wu, Yu and Zhang, Ziqiang and Zhou, Long and Liu, Shujie and Chen, Zhuo and Liu, Yanqing and Wang, Huaming and Li, Jinyu and others},
  journal = {arXiv preprint arXiv:2301.02111},
  year    = {2023}
}
@article{chameleon2024,
  title   = {Chameleon: Mixed-Modal Early-Fusion Foundation Models},
  author  = {Chameleon Team},
  journal = {arXiv preprint arXiv:2405.09818},
  year    = {2024}
}
@article{zhu2020deformable,
  title   = {Deformable detr: Deformable transformers for end-to-end object detection},
  author  = {Zhu, Xizhou and Su, Weijie and Lu, Lewei and Li, Bin and Wang, Xiaogang and Dai, Jifeng},
  journal = {arXiv preprint arXiv:2010.04159},
  year    = {2020}
}

@article{huang2026step3,
  title   = {STEP3-VL-10B Technical Report},
  author  = {Huang, Ailin and Yao, Chengyuan and Han, Chunrui and Wan, Fanqi and Guo, Hangyu and Lv, Haoran and Zhou, Hongyu and Wang, Jia and Zhou, Jian and Sun, Jianjian and others},
  journal = {arXiv preprint arXiv:2601.09668},
  year    = {2026}
}
@article{wei2024small,
  title   = {Small language model meets with reinforced vision vocabulary},
  author  = {Wei, Haoran and Kong, Lingyu and Chen, Jinyue and Zhao, Liang and Ge, Zheng and Yu, En and Sun, Jianjian and Han, Chunrui and Zhang, Xiangyu},
  journal = {arXiv preprint arXiv:2401.12503},
  year    = {2024}
}

@inproceedings{liu2021wb,
  title     = {WB-DETR: Transformer-Based Detector Without Backbone},
  author    = {Liu, Fanfan and Wei, Haoran and Zhao, Wenzhe and Li, Guozhen and Peng, Jingquan and Li, Zihao},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages     = {2979--2987},
  year      = {2021}
}

@article{deepseekv3,
  title   = {Deepseek-v3 technical report},
  author  = {Liu, Aixin and Feng, Bei and Xue, Bing and Wang, Bingxuan and Wu, Bochao and Lu, Chengda and Zhao, Chenggang and Deng, Chengqi and Zhang, Chenyu and Ruan, Chong and others},
  journal = {arXiv preprint arXiv:2412.19437},
  year    = {2024}
}

@article{sun2025pp,
  title   = {PP-DocLayout: A Unified Document Layout Detection Model to Accelerate Large-Scale Data Construction},
  author  = {Sun, Ting and Cui, Cheng and Du, Yuning and Liu, Yi},
  journal = {arXiv preprint arXiv:2503.17213},
  year    = {2025}
}
@article{wang2024mineru,
  title   = {Mineru: An open-source solution for precise document content extraction},
  author  = {Wang, Bin and Xu, Chao and Zhao, Xiaomeng and Ouyang, Linke and Wu, Fan and Zhao, Zhiyuan and Xu, Rui and Liu, Kaiwen and Qu, Yuan and Shang, Fukai and others},
  journal = {arXiv preprint arXiv:2409.18839},
  year    = {2024}
}
@article{poznanski2025olmocr,
  title   = {olmocr: Unlocking trillions of tokens in pdfs with vision language models},
  author  = {Poznanski, Jake and Rangapur, Aman and Borchardt, Jon and Dunkelberger, Jason and Huff, Regan and Lin, Daniel and Wilhelm, Christopher and Lo, Kyle and Soldaini, Luca},
  journal = {arXiv preprint arXiv:2502.18443},
  year    = {2025}
}

@article{schuhmann2021laion,
  title   = {Laion-400m: Open dataset of clip-filtered 400 million image-text pairs},
  author  = {Schuhmann, Christoph and Vencu, Richard and Beaumont, Romain and Kaczmarczyk, Robert and Mullis, Clayton and Katta, Aarush and Coombes, Theo and Jitsev, Jenia and Komatsuzaki, Aran},
  journal = {arXiv preprint arXiv:2111.02114},
  year    = {2021}
}

@article{gu2022wukong,
  title   = {Wukong: A 100 million large-scale chinese cross-modal pre-training benchmark},
  author  = {Gu, Jiaxi and Meng, Xiaojun and Lu, Guansong and Hou, Lu and Minzhe, Niu and Liang, Xiaodan and Yao, Lewei and Huang, Runhui and Zhang, Wei and Jiang, Xin and others},
  journal = {Advances in Neural Information Processing Systems},
  volume  = {35},
  pages   = {26418--26431},
  year    = {2022}
}

@article{cui2025paddleocr,
  title   = {Paddleocr 3.0 technical report},
  author  = {Cui, Cheng and Sun, Ting and Lin, Manhui and Gao, Tingquan and Zhang, Yubo and Liu, Jiaxuan and Wang, Xueqing and Zhang, Zelun and Zhou, Changda and Liu, Hongen and others},
  journal = {arXiv preprint arXiv:2507.05595},
  year    = {2025}
}

@inproceedings{chen2024onechart,
  title     = {Onechart: Purify the chart structural extraction via one auxiliary token},
  author    = {Chen, Jinyue and Kong, Lingyu and Wei, Haoran and Liu, Chenglong and Ge, Zheng and Zhao, Liang and Sun, Jianjian and Han, Chunrui and Zhang, Xiangyu},
  booktitle = {Proceedings of the 32nd ACM International Conference on Multimedia},
  pages     = {147--155},
  year      = {2024}
}

@article{wei2024slow,
  title   = {Slow Perception: Let's Perceive Geometric Figures Step-by-step},
  author  = {Wei, Haoran and Yin, Youyang and Li, Yumeng and Wang, Jia and Zhao, Liang and Sun, Jianjian and Ge, Zheng and Zhang, Xiangyu and Jiang, Daxin},
  journal = {arXiv preprint arXiv:2412.20631},
  year    = {2024}
}
@article{wu2024deepseek,
  title   = {Deepseek-vl2: Mixture-of-experts vision-language models for advanced multimodal understanding},
  author  = {Wu, Zhiyu and Chen, Xiaokang and Pan, Zizheng and Liu, Xingchao and Liu, Wen and Dai, Damai and Gao, Huazuo and Ma, Yiyang and Wu, Chengyue and Wang, Bingxuan and others},
  journal = {arXiv preprint arXiv:2412.10302},
  year    = {2024}
}
@article{OPT-IML,
  title   = {OPT-IML: Scaling Language Model Instruction Meta Learning through the Lens of Generalization},
  author  = {Iyer, Srinivasan and Lin, Xi Victoria and Pasunuru, Ramakanth and Mihaylov, Todor and Simig, D{\'a}niel and Yu, Ping and Shuster, Kurt and Wang, Tianlu and Liu, Qing and Koura, Punit Singh and others},
  journal = {arXiv preprint arXiv:2212.12017},
  year    = {2022}
}
@inproceedings{AdamW,
  author    = {Ilya Loshchilov and
               Frank Hutter},
  title     = {Decoupled Weight Decay Regularization},
  booktitle = {{ICLR}},
  year      = {2019}
}
@article{loshchilov2016sgdr,
  title   = {Sgdr: Stochastic gradient descent with warm restarts},
  author  = {Loshchilov, Ilya and Hutter, Frank},
  journal = {arXiv preprint arXiv:1608.03983},
  year    = {2016}
}

@misc{highflyer2023haillm,
  author = {High-flyer},
  title  = {{HAI-LLM}: Efficient and lightweight training tool for large models},
  year   = {2023},
  url    = {https://www.high-flyer.cn/en/blog/hai-llm}
}
@article{feng2025dolphin,
  title   = {Dolphin: Document image parsing via heterogeneous anchor prompting},
  author  = {Feng, Hao and Wei, Shu and Fei, Xiang and Shi, Wei and Han, Yingdong and Liao, Lei and Lu, Jinghui and Wu, Binghong and Liu, Qi and Lin, Chunhui and others},
  journal = {arXiv preprint arXiv:2505.14059},
  year    = {2025}
}

@misc{marker,
  author = {},
  title  = {Marker},
  year   = {},
  url    = {https://github.com/datalab-to/marker}
}

@article{liu2020multilingual,
  title     = {Multilingual denoising pre-training for neural machine translation},
  author    = {Liu, Yinhan and Gu, Jiatao and Goyal, Naman and Li, Xian and Edunov, Sergey and Ghazvininejad, Marjan and Lewis, Mike and Zettlemoyer, Luke},
  journal   = {Transactions of the Association for Computational Linguistics},
  volume    = {8},
  pages     = {726--742},
  year      = {2020},
  publisher = {MIT Press}
}
@misc{mathpix,
  author = {},
  title  = {Mathpix},
  year   = {},
  url    = {https://mathpix.com/}
}
@inproceedings{he2016deep,
  title     = {Deep residual learning for image recognition},
  author    = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages     = {770--778},
  year      = {2016}
}
@article{li2025monkeyocr,
  title   = {MonkeyOCR: Document Parsing with a Structure-Recognition-Relation Triplet Paradigm},
  author  = {Li, Zhang and Liu, Yuliang and Liu, Qiang and Ma, Zhiyin and Zhang, Ziyang and Zhang, Shuo and Guo, Zidun and Zhang, Jiarui and Wang, Xinyu and Bai, Xiang},
  journal = {arXiv preprint arXiv:2506.05218},
  year    = {2025}
}
@article{nassar2025smoldocling,
  title   = {SmolDocling: An ultra-compact vision-language model for end-to-end multi-modal document conversion},
  author  = {Nassar, Ahmed and Marafioti, Andres and Omenetti, Matteo and Lysak, Maksym and Livathinos, Nikolaos and Auer, Christoph and Morin, Lucas and de Lima, Rafael Teixeira and Kim, Yusik and Gurbuz, A Said and others},
  journal = {arXiv preprint arXiv:2503.11576},
  year    = {2025}
}

@article{Qwen2.5-VL,
  title   = {Qwen2.5-VL Technical Report},
  author  = {Bai, Shuai and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Song, Sibo and Dang, Kai and Wang, Peng and Wang, Shijie and Tang, Jun and Zhong, Humen and Zhu, Yuanzhi and Yang, Mingkun and Li, Zhaohai and Wan, Jianqiang and Wang, Pengfei and Ding, Wei and Fu, Zheren and Xu, Yiheng and Ye, Jiabo and Zhang, Xi and Xie, Tianbao and Cheng, Zesen and Zhang, Hang and Yang, Zhibo and Xu, Haiyang and Lin, Junyang},
  journal = {arXiv preprint arXiv:2502.13923},
  year    = {2025}
}
@article{cui2025paddleocrvl,
  title   = {Paddleocr-vl: Boosting multilingual document parsing via a 0.9 b ultra-compact vision-language model},
  author  = {Cui, Cheng and Sun, T and Liang, S and others},
  journal = {arXiv preprint arXiv:2510.14528},
  year    = {2025}
}
@misc{ocrflux,
  author = {},
  title  = {OCRFlux},
  year   = {2025},
  url    = {https://github.com/chatdoc-com/OCRFlux}
}

@misc{OCRVerse,
  author = {},
  title  = {OCRVerse},
  year   = {2025},
  url    = {https://github.com/DocTron-hub/OCRVerse}
}


@misc{NanonetsOCRs,
  author = {},
  title  = {Nanonets-OCR-s},
  year   = {2025},
  url    = {https://huggingface.co/nanonets/Nanonets-OCR-s}
}

@misc{GPT4,
  title  = {GPT-4 Technical Report},
  author = {OpenAI},
  year   = {2023},
  eprint = {arXiv preprint arXiv:2303.08774}
}

@article{zhu2025internvl3,
  title   = {Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models},
  author  = {Zhu, Jinguo and Wang, Weiyun and Chen, Zhe and Liu, Zhaoyang and Ye, Shenglong and Gu, Lixin and Tian, Hao and Duan, Yuchen and Su, Weijie and Shao, Jie and others},
  journal = {arXiv preprint arXiv:2504.10479},
  year    = {2025}
}


@misc{dots,
  title  = {dots.ocr},
  author = {Rednote},
  year   = {2025},
  url    = {https://github.com/rednote-hilab/dots.ocr}
}

@misc{google_gemini_web,
  author = {Google AI},
  title  = {Gemini 2.5-Pro},
  year   = {2025},
  url    = {https://gemini.google.com/}
}

@article{deepseek2024,
  title={DeepSeek OCR 2: Advanced Optical Character Recognition with LLM Integration},
  author={DeepSeek Team},
  journal={arXiv preprint arXiv:2401.00001},
  year={2024}
}

@article{epstein2026,
  title={Epstein Archive Analysis: Multi-modal Approach for Confidential Document Processing},
  author={Research Team},
  journal={Journal of Digital Forensics},
  year={2026}
}

@article{llm2023,
  title={Large Language Models for Named Entity Recognition in Sensitive Documents},
  author={AI Research Group},
  journal={ACL Anthology},
  year={2023}
}

% 以下为新增的中文参考文献条目

@phdthesis{ji2017complex,
  author = {吉红宇},
  title  = {基于复杂网络分析的人物关系挖掘},
  school = {电子科技大学},
  address = {成都},
  year   = {2017}
}

@phdthesis{zhang2018perception,
  author = {张荣杰},
  title  = {感知与现实人际关系网络：经典文学作品分析},
  school = {闽南师范大学},
  address = {漳州},
  year   = {2018}
}

@article{chen2015dream,
  author  = {陈蕾 and 胡亦旻 and 艾苇 and others},
  title   = {红楼梦中社会权势关系的提取及网络构建},
  journal = {中文信息学报},
  year    = {2015},
  volume  = {29},
  number  = {5},
  pages   = {185--203}
}

@article{zhao2017chinese,
  author  = {赵京胜 and 张丽 and 朱巧明 and others},
  title   = {中文文学作品中的社会网络抽取与分析},
  journal = {中文信息学报},
  year    = {2017},
  volume  = {31},
  number  = {2},
  pages   = {99--106,116}
}

@article{dong2018corpus,
  author  = {董晓烨 and 柴静},
  title   = {语料库辅助的文学作品主题分析},
  journal = {西安电子科技大学学报(社会科学版)},
  year    = {2018},
  volume  = {28},
  number  = {3},
  pages   = {106--111}
}

@article{lou2018novel,
  author  = {楼锴毅 and 霸元婕 and 李绍昂},
  title   = {基于社交网络的小说聚类},
  journal = {软件工程},
  year    = {2018},
  volume  = {21},
  number  = {10},
  pages   = {14--16}
}

@phdthesis{tu2019similarity,
  author = {涂轶文},
  title  = {基于人物相似度的互联网络人物关系分析方法研究},
  school = {电子科技大学},
  address = {成都},
  year   = {2019}
}

@article{lin2018dream_structure,
  author  = {林峰 and 赵广平 and 林娜 and others},
  title   = {《红楼梦》文本的社会网络结构分析},
  journal = {石家庄铁道大学学报(社会科学版)},
  year    = {2018},
  volume  = {12},
  number  = {1},
  pages   = {58--63}
}

@article{tang2018water_margin,
  author  = {唐毅 and 王硕 and 胡桓},
  title   = {《水浒传》人物关系网络的文本挖掘},
  journal = {社科纵横},
  year    = {2018},
  volume  = {33},
  number  = {4},
  pages   = {117--120}
}

@phdthesis{li2019cooccurrence,
  author = {李娇},
  title  = {基于共现与关联挖掘的人物关系图谱研究与实现},
  school = {西北民族大学},
  address = {兰州},
  year   = {2019}
}

@article{hu2009behavior,
  author  = {胡岚曦},
  title   = {一种基于行为分析的人物关系网络发掘方法},
  journal = {计算机应用与软件},
  year    = {2009},
  volume  = {26},
  number  = {10},
  pages   = {256--258}
}

@article{ren2015water_margin,
  author  = {任东升 and 马婷},
  title   = {基于语料库的《水浒传》沙博理英译本意合句式研究},
  journal = {外语研究},
  year    = {2015},
  volume  = {149},
  number  = {1},
  pages   = {64--70}
}

@article{du2016water_margin_tea,
  author  = {杜贵晨},
  title   = {《水浒传》茶事考论},
  journal = {陕西理工学院学报(社会科学版)},
  year    = {2016},
  volume  = {3},
  number  = {4},
  pages   = {1--10}
}

@article{li2020water_margin_anger,
  author  = {李桂奎},
  title   = {论《水浒传》"怒气"摹写之"乖错"情理},
  journal = {中原文化研究},
  year    = {2020},
  volume  = {8},
  number  = {3},
  pages   = {92--100}
}

@article{li2019complex_ml,
  author  = {李泽荃 and 杨曌 and 刘嵘 and others},
  title   = {复杂网络与机器学习融合的研究进展},
  journal = {计算机应用与软件},
  year    = {2019},
  volume  = {36},
  number  = {4},
  pages   = {11--28,62}
}

@article{xiong2018olap,
  author  = {熊中敏 and 朱春卫 and 郭振辉 and others},
  title   = {基于OLAP和聚类分析的关联规则挖掘方法},
  journal = {计算机应用与软件},
  year    = {2018},
  volume  = {35},
  number  = {5},
  pages   = {58--61}
}

@phdthesis{si2014link,
  author = {司帅宗},
  title  = {社会网络中的链路预测及网络重构},
  school = {东北大学},
  address = {沈阳},
  year   = {2014}
}

% 以下为新增的学位论文条目

@phdthesis{tian2025deep,
  author = {田秀敏},
  title  = {深度学习在人物关系抽取中的应用研究},
  school = {西安电子科技大学},
  year   = {2025}
}

@phdthesis{zhang2025semantic,
  author = {张宇桐},
  title  = {基于语义的篇章级人物关系抽取的研究},
  school = {东北大学},
  year   = {2025}
}

@phdthesis{zhu2025bert,
  author = {朱喜超},
  title  = {基于BERT的中文人物关系抽取方法研究},
  school = {中原工学院},
  year   = {2025}
}

@phdthesis{wang2025social,
  author = {王梦妮},
  title  = {基于大规模社交网络数据的人物关系知识图谱构建技术研究},
  school = {北京邮电大学},
  year   = {2025}
}

@phdthesis{zhang2025supervised,
  author = {张嘉璐},
  title  = {基于有监督深度学习的知识图谱构建的研究与实现},
  school = {北京邮电大学},
  year   = {2025}
}

@phdthesis{hu2024graph,
  author = {胡楠},
  title  = {人物关系图谱构建技术研究与实践},
  school = {郑州大学},
  year   = {2024}
}
@article{2017An,
  title   = {An End-to-End Trainable Neural Network for Image-Based Sequence Recognition and Its Application to Scene Text Recognition},
  author  = { Shi, Baoguang  and  Bai, Xiang  and  Yao, Cong },
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year    = {2017}
}

@phdthesis{qin2023deep,
  author = {秦苗苗},
  title  = {基于深度学习的人物关系抽取方法研究与应用},
  school = {郑州轻工业大学},
  year   = {2023}
}

@phdthesis{zhang2022chinese_dl,
  author = {张闽珺},
  title  = {基于深度学习的中文人物关系抽取研究},
  school = {东华大学},
  year   = {2022}
}

@phdthesis{lei2022chinese_app,
  author = {雷西唯},
  title  = {基于深度学习的中文人物关系抽取研究与应用},
  school = {青海师范大学},
  year   = {2022}
}

@phdthesis{wen2021classics,
  author = {温枫杰},
  title  = {基于深度学习的中华典籍人物关系研究},
  school = {中北大学},
  year   = {2021}
}

% 以下为新增的爱泼斯坦档案相关在线资源条目

@misc{doj_epstein_disclosures,
  author = {{United States Department of Justice}},
  title  = {DOJ Disclosures - Epstein Files},
  year   = {2025},
  url    = {https://www.justice.gov/epstein/doj-disclosures},
  note   = {美国司法部披露的爱泼斯坦文件}
}

@misc{courier_epstein_database,
  author = {{COURIER Newsroom}},
  title  = {Searchable Database of Epstein Estate Files},
  year   = {2025},
  url    = {https://couriernewsroom.com/news/we-created-a-searchable-database-with-all-20000-files-from-epsteins-estate/},
  note   = {COURIER编纂的20,000份爱泼斯坦遗产文件可检索资料库},
  howpublished = {Online Database}
}
