%!TeX program = xelatex
\documentclass[12pt,hyperref,a4paper,UTF8]{ctexart}
\usepackage{SDAUReport}

%%-------------------------------正文开始---------------------------%%
\begin{document}

%%-----------------------封面--------------------%%
\cover
\thispagestyle{empty} % 首页不显示页码
%%------------------摘要-------------%%
\newpage
\begin{abstract}

    爱泼斯坦档案作为牵扯全球顶级权贵圈的重要机密档案，包含海量扫描件形式的文本与图片信息，其人物关系的挖掘对事件真相还原具有重要意义。随着光学字符识别（OCR）技术与大语言模型（LLM）、LLM视觉模型的快速发展，为海量非结构化档案的智能化分析提供了技术支撑。本文以爱泼斯坦档案为研究对象，探究基于LLM视觉模型的人物关系分析可行性，通过调用DeepSeek OCR 2完成档案扫描件的文本化处理，利用DeepSeek V3.2大语言模型进行文本层面的人物关系抽取，借助DeepSeek OCR 2的LLM视觉相关功能实现档案图片的内容描述、要素识别与提取，并结合文本与图片信息进行关联分析。研究结果表明，OCR技术能高效完成档案文本化转换，LLM可精准抽取文本中的人物关系，LLM视觉模型能有效捕捉图片中的人物及场景要素，三者结合可实现爱泼斯坦档案人物关系的多维度、高精度分析，验证了该研究方案的可行性，同时指出了模型应用中的不足并提出优化方向，为同类机密档案的智能化人物关系分析提供参考。


    \vspace{1em}
    \noindent\textbf{关键词：}LLM视觉模型；OCR技术；人物关系抽取；文本识别；图片分析

    \newpage

    \begin{center}
        \textbf{Abstract}
    \end{center}

    The Epstein archive, as an important confidential archive involving the world's top powerful circles, contains a large amount of text and image information in the form of scanned copies. The excavation of its character relationships is of great significance for restoring the truth of the incident. With the rapid development of Optical Character Recognition (OCR) technology, Large Language Models (LLMs), and LLM vision models, it provides technical support for the intelligent analysis of massive unstructured archives. Taking the Epstein archive as the research object, this paper explores the feasibility of character relationship analysis based on LLM vision models. It completes the textual processing of archive scanned copies by calling DeepSeek OCR 2, uses the DeepSeek V3.2 large language model for character relationship extraction at the text level, and realizes content description, element identification and extraction of archive images with the help of LLM vision-related functions of DeepSeek OCR 2, and conducts correlation analysis combined with text and image information. The research results show that OCR technology can efficiently complete the textual conversion of archives, LLM can accurately extract character relationships from text, and LLM vision model can effectively capture characters and scene elements in images. The combination of the three can realize multi-dimensional and high-precision analysis of character relationships in the Epstein archive, verifying the feasibility of the research scheme. At the same time, it points out the deficiencies in the model application and puts forward optimization directions, providing a reference for the intelligent character relationship analysis of similar confidential archives.

    \noindent\textbf{Key words:} LLM Vision Model; OCR Technology;  Character Relationship Extraction; Text Recognition; Image Analysis

\end{abstract}

%%--------------------------目录页------------------------%%
\newpage
\tableofcontents

%%------------------------正文页从这里开始-------------------%
\newpage

\section{简介}

\subsection{研究背景}

在数字化时代，海量非结构化数据（如扫描件、图片、手写文档）的高效处理成为人工智能领域的研究热点，其中光学字符识别（OCR）技术作为非结构化文本转换为结构化文本的核心技术，经历了从传统模板匹配到深度学习驱动的跨越式发展。\cite{goyal2017making}传统OCR技术受限于字体、清晰度、布局等因素，识别准确率和泛化能力不足，而近年来基于深度学习和大语言模型（LLM）的OCR技术不断迭代，结合上下文理解和视觉特征分析，显著提升了复杂场景下的文本识别效果，尤其适用于老旧档案、模糊扫描件等特殊文本的处理。与此同时，LLM及其衍生的LLM视觉模型（多模态模型）快速崛起，实现了文本、图片等多模态信息的融合理解，打破了单一模态分析的局限，为跨模态档案分析提供了全新技术路径，其中DeepSeek系列OCR模型和大语言模型凭借高效的处理能力和精准的识别效果，在非结构化数据处理中得到广泛应用。\cite{liu2020multilingual}2026年1月27日，DeepSeek OCR 2发布，开源视觉模型的质量和性能迈上新台阶。



爱泼斯坦事件是2019年以来震惊全球的权贵性丑闻事件，美国金融家杰弗里·爱泼斯坦因涉嫌性贩卖等多项罪名被捕，后在狱中离奇死亡，其留下的海量档案成为还原事件真相、揭露背后权贵网络的关键。\cite{courier_epstein_database}2026年1月30日，美国司法部依据《爱波斯坦档案透明法案》公开超300万页涉案文件，含2000余段视频、18万张图片，涉及马斯克、比尔·盖茨、克林顿、特朗普、安德鲁王子等全球顶级权贵人物，涵盖电邮来往、行程安排、私密照片等多种类型的信息。这起横跨十余年的未成年人性交易与权贵勾结大案全面引爆。案件始于2005年佛罗里达州少女举报，2008年爱波斯坦以轻罪轻判逍遥法外；2019年他再度被捕后于狱中离奇死亡，官方结论疑点重重，同伙麦克斯韦尔2022年获刑，成为少数被追责者。截至2月3日，文件持续发酵，欧美政要接连辞职、王室与前总统被传唤、海量私密影像与往来记录曝光，同时出现未打码受害者照片泄露、半数档案永久封存、司法部紧急下架文件等争议，司法遮羞与权力博弈彻底暴露，一场由金钱、权力与性犯罪编织的跨国黑幕，被强行拉到公众面前。\cite{doj_epstein_disclosures}该档案多以扫描件形式留存，包含大量模糊文本、手写批注和私密图片，文本内容繁杂，图片信息隐蔽，人工分析不仅耗时耗力，且易出现信息遗漏和判断偏差，难以快速挖掘档案中隐藏的复杂人物关系，而档案的公开过程充满波折，部分文件被大量修改、涂黑，进一步增加了分析难度，亟需高效、精准的智能化技术手段介入处理。

传统档案人物关系分析多依赖人工整理和手动标注，效率低下且主观性强，难以适应海量、复杂档案的分析需求。\cite{tian2025deep}随着大模型技术的不断成熟，利用LLM进行文本语义分析、命名实体识别和人物关系抽取，利用LLM视觉模型进行图片内容理解和要素提取，已成为档案智能化分析的重要方向。目前，大模型已被应用于历史档案、法律档案等领域的分析，通过多模态融合技术实现档案信息的深度挖掘，但针对爱泼斯坦这类高敏感度、多模态、复杂关联的机密档案，尚未形成成熟的智能化人物关系分析方案。基于此，结合OCR技术、LLM和LLM视觉模型，开展爱泼斯坦档案人物关系分析的可行性研究，不仅能解决该档案分析中的技术难题，还原事件背后的权贵网络，也能为同类复杂机密档案的智能化处理提供技术参考和实践借鉴，推动档案智能化分析技术的落地应用。

\subsection{研究目的}

本文以爱泼斯坦档案为研究载体，结合OCR技术、LLM和LLM视觉模型，开展人物关系分析可行性研究，具体研究目的包括三个方面：

第一，探究OCR技术在爱泼斯坦档案文本化处理中的应用可行性与效果。以DeepSeek OCR 2为核心工具，处理爱泼斯坦档案的扫描件（含清晰文本、模糊文本、手写批注等），完成非结构化扫描件到结构化文本的转换，分析OCR技术在复杂档案文本识别中的准确率、效率及存在的问题，验证其在档案文本化处理中的适用性，为后续人物关系抽取提供高质量的文本数据支撑。

第二，探究大语言模型在爱泼斯坦档案文本分析中的应用可行性与效果。调用DeepSeek V3.2大语言模型，对OCR转换后的结构化文本进行预处理和语义分析，完成文本中命名实体（人物、地点、事件等）的识别，抽取人物之间的关联关系（如社交往来、利益关联、行程交集等），分析大语言模型在复杂文本、敏感信息文本中人物关系抽取的精准度，验证其在档案文本语义分析中的有效性。

第三，探究LLM视觉模型在爱泼斯坦档案图片分析中的应用可行性与效果。借助DeepSeek OCR 2的LLM视觉相关功能，对档案中的图片信息进行内容描述和要素识别与提取，识别人物身份、场景信息、物品关联等，结合文本上下文进行关联分析，补充文本层面未捕捉到的人物关系细节，验证LLM视觉模型在档案图片分析、人物关系补充挖掘中的作用，实现文本与图片多模态融合的人物关系分析。

\subsection{国内外研究现状}

\subsubsection{OCR识别技术研究现状}

国外OCR识别技术起步较早，经过多年发展已形成较为成熟的技术体系，早期以IBM、Google等企业为核心，推出了基于模板匹配和统计学习的OCR产品，随后逐步融入深度学习技术，提升了复杂场景下的识别效果。近年来，国外研究重点转向LLM与OCR技术的融合，利用LLM的上下文理解能力，解决模糊文本、手写文本、复杂布局文本的识别难题，如Google Cloud Vision OCR结合大语言模型，实现了多语言、复杂场景下的文本识别和语义理解，在档案、古籍等文本处理中得到应用；Microsoft Azure OCR优化了深度学习模型架构，提升了低清晰度扫描件的文本识别准确率，支持表格、公式等特殊文本的提取。同时，国外研究注重OCR技术与多模态模型的结合，实现文本与图片信息的同步识别和关联分析，在机密档案、历史文献处理中展现出良好的应用前景。

国内OCR识别技术近年来发展迅速，形成了以互联网企业、科研机构为核心的研究团队，聚焦于深度学习与OCR技术的融合创新，在复杂文本识别、多语言识别等方面达到国际先进水平。国内研究重点集中在老旧档案、手写文本等特殊场景的OCR优化，如百度飞桨OCR\cite{cui2025paddleocr}、阿里 Qwen VL\cite{bai2025qwen3vltechnicalreport}等产品，结合深度学习模型，提升了模糊、倾斜、破损文本的识别效果；DeepSeek团队推出的DeepSeek OCR系列模型，创新性引入上下文光学压缩技术和LLM架构，实现了高效、精准的文本识别，其中DeepSeek OCR 2引入具备因果推理能力的编码器DeepEncoder V2，智能对视觉token进行重新排序，在保持91.09\%高准确率的同时，大幅降低token消耗，适用于海量档案扫描件的高效处理，相比传统OCR模型和其他主流OCR模型具有显著优势。\cite{wei2026deepseek}此外，国内研究注重OCR技术与行业场景的结合，在档案管理、法律文书处理等领域实现了规模化应用，但针对高敏感度、多模态复杂机密档案的OCR处理研究仍相对较少，相关技术仍需进一步优化。
\begin{figure}[!htbp]
    \centering
    \includegraphics[width =.5\textwidth]{figures/ocryingyong.pdf}
    \caption{OCR 应用场景}
    \label{OCR应用场景}
\end{figure}

\subsubsection{命名实体识别技术研究现状}

命名实体识别（NER）是人物关系抽取的基础，其核心是从文本中识别出具有特定意义的实体（如人物、地点、组织、事件等），近年来随着大语言模型的发展，NER技术实现了跨越式提升。国外命名实体识别研究起步较早，早期以基于规则和统计学习的方法为主，如隐马尔可夫模型（HMM）、条件随机场（CRF）等，但其泛化能力不足，难以适应复杂文本场景。近年来，国外研究重点转向基于深度学习和LLM的NER技术，利用Transformer架构、BERT等预训练模型，结合上下文语义理解，提升了NER的准确率和泛化能力，尤其在模糊实体、多义实体识别中表现突出。例如，Google的BERT-NER模型、OpenAI的GPT系列模型，通过预训练和微调，可快速适应不同领域的文本NER任务，在档案、新闻、法律等文本处理中得到广泛应用，能够精准识别文本中的人物实体及其相关属性信息。

国内命名实体识别研究近年来紧跟国际前沿，聚焦于深度学习模型的优化和行业场景的适配，形成了具有自主知识产权的NER技术和产品。国内研究重点解决中文文本、混合语言文本的NER难题，同时注重多模态NER技术的研究，结合文本和图片信息，提升实体识别的完整性和准确性。例如，清华大学、哈尔滨工业大学等科研机构，基于BERT、ERNIE等预训练模型，优化了NER模型的架构，提升了复杂文本中命名实体的识别效果；DeepSeek V3.2大语言模型集成了高效的NER功能，能够快速识别文本中的人物、地点、事件等实体，支持多语言实体识别，适用于爱泼斯坦档案这类混合语言、敏感信息丰富的文本处理。目前，国内外NER技术已相对成熟，但针对高敏感度、多歧义、上下文复杂的机密档案，实体识别的精准度仍有提升空间，尤其是手写批注、模糊文本中的实体识别，仍是当前研究的难点问题。\cite{ji2017complex,zhao2017chinese,li2019cooccurrence}

\subsubsection{人物关系抽取技术研究现状}

人物关系抽取是文本语义分析的核心任务之一，其核心是从文本中挖掘出人物之间的关联关系，近年来随着LLM和多模态技术的发展，人物关系抽取技术逐步从单一文本抽取向多模态融合抽取转型。国外人物关系抽取研究起步较早，早期以基于规则和统计学习的方法为主，通过人工制定规则、统计实体共现频率等方式，挖掘人物之间的关系，但这类方法效率低下、泛化能力差，难以适应复杂文本场景。近年来，国外研究重点转向基于深度学习和LLM的人物关系抽取技术，利用Transformer架构、图神经网络（GNN）等模型，结合上下文语义理解和知识图谱，提升了人物关系抽取的精准度和泛化能力，同时注重多模态融合的人物关系抽取研究，结合文本和图片信息，挖掘隐藏的人物关系。例如，斯坦福大学、麻省理工学院等科研机构，提出了基于LLM的人物关系抽取模型，通过微调预训练模型，实现了复杂文本中人物关系的精准抽取；同时，结合LLM视觉模型，从图片中提取人物特征和场景信息，补充文本层面未捕捉到的人物关系，在社交网络、档案分析等领域得到应用，相关研究已形成较为完善的技术体系，能够处理多类型、复杂关联的人物关系抽取任务，如融合角色指代的多方对话关系抽取模型，显著提升了对话类文本中人物关系的抽取效果。

国内人物关系抽取研究近年来发展迅速，聚焦于行业场景的适配和模型优化，在中文文本、复杂档案等领域的研究具有一定优势。国内研究重点解决复杂文本、多模态数据中的人物关系抽取难题，结合知识图谱和上下文语义理解，提升关系抽取的精准度，同时注重模型的轻量化和高效性，便于落地应用。例如，百度、阿里等企业，推出了基于自有预训练模型的人物关系抽取工具，适用于新闻、档案等文本处理；科研机构则聚焦于多模态人物关系抽取研究，结合OCR技术、LLM视觉模型，实现了文本与图片信息的融合，挖掘隐藏的人物关系。DeepSeek系列模型通过多模态融合技术，可实现文本中人物关系的抽取和图片中人物要素的识别，为多模态人物关系分析提供了技术支撑，但针对爱泼斯坦档案这类高敏感度、多模态、复杂关联的机密档案，人物关系抽取的研究仍相对匮乏，尚未形成成熟的技术方案，相关模型在敏感信息处理、模糊关系挖掘等方面仍需进一步优化，同时如何将文本与图片中的人物关系进行有效关联，也是当前研究的重点和难点问题。\cite{hu2009behavior,xiong2018olap,tian2025deep,zhu2025bert,zhang2025semantic,hu2024graph}

\subsection{研究内容及目标}

\subsubsection{研究内容}

本文围绕基于LLM视觉模型的爱泼斯坦档案人物关系分析可行性展开研究，具体研究内容如下：

\begin{enumerate}
    \item 相关技术综述：系统梳理OCR识别技术（重点是DeepSeek OCR系列）、人物关系抽取技术的核心原理和发展现状，重点分析基于上下文光学压缩的DeepSeek OCR和基于LLM架构的DeepSeek OCR 2的技术特点，以及基于规则和深度学习的人物关系抽取方法的优劣，为后续研究提供理论和技术支撑。

    \item 研究方法设计：结合爱泼斯坦档案的特点，设计基于Free OCR、LLM和LLM视觉模型的人物关系分析方案，明确各技术的应用流程，包括调用DeepSeek OCR 2的FreeOCR功能完成档案文本化处理，利用DeepSeek V3.2大语言模型完成文本分析和人物关系抽取，借助DeepSeek OCR 2的LLM视觉功能完成图片描述、要素识别与提取，以及文本与图片信息的关联分析方法。

    \item 研究过程及分析：按照设计的研究方法，开展爱泼斯坦档案人物关系分析的实证研究，分四个阶段完成：利用OCR技术对档案扫描件进行文本化处理，分析处理效果；利用大语言模型对文本数据进行人物关系抽取，分析抽取精准度；利用LLM视觉模型对图片进行描述和要素提取，分析图片分析效果；结合文本和图片信息进行关联分析，完善人物关系挖掘结果，全面验证研究方案的可行性。

    \item 结论与展望：总结本次研究的核心结论，明确基于LLM视觉模型的爱泼斯坦档案人物关系分析的可行性和优势，指出研究过程中存在的问题和不足，提出后续的优化方向和研究展望，为同类档案的智能化人物关系分析提供参考。
\end{enumerate}

\subsubsection{研究目标}

\begin{figure}[!htbp]
    \centering
    \includegraphics[width =.5\textwidth]{figures/框架.png}
    \caption{框架图}
    \label{框架}
\end{figure}


本文的研究目标主要包括：

\begin{enumerate}
    \item 验证OCR技术（DeepSeek OCR 2）在爱泼斯坦档案文本化处理中的可行性，实现档案扫描件到结构化文本的高效、精准转换，为人物关系抽取提供高质量文本数据。

    \item 验证大语言模型（DeepSeek V3.2）在爱泼斯坦档案文本分析、人物关系抽取中的可行性，实现文本中人物实体的精准识别和人物关系的有效抽取，明确其在复杂文本处理中的优势和不足。

    \item 验证LLM视觉模型（DeepSeek OCR 2）在爱泼斯坦档案图片分析中的可行性，实现图片内容的精准描述和人物、场景等要素的有效提取，补充文本层面的人物关系信息，实现多模态融合的人物关系分析。

    \item 形成一套基于LLM视觉模型的爱泼斯坦档案人物关系分析方案，明确各技术的应用流程和效果，验证该方案的可行性和实用性，为同类复杂机密档案的智能化人物关系分析提供技术参考和实践借鉴。
\end{enumerate}



\section{综述}

\subsection{OCR识别技术}

OCR（Optical Character Recognition，光学字符识别）技术是指通过光学设备捕捉图像中的文本信息，利用计算机算法将其转换为可编辑、可检索的结构化文本的技术，其核心是实现非结构化图像文本到结构化文本的转换，是海量档案、图片等非结构化数据智能化处理的基础。\cite{2017An}随着深度学习和大语言模型的发展，OCR技术已从传统的单一文本识别，逐步向上下文理解、多模态融合识别转型，其中DeepSeek OCR系列模型凭借其高效的处理能力和精准的识别效果，在复杂档案处理中展现出显著优势，本文重点研究基于上下文光学压缩的DeepSeek OCR和基于LLM架构的DeepSeek OCR 2两种模型。

\subsubsection{基于上下文光学压缩的DeepSeek OCR}

基于上下文光学压缩的DeepSeek OCR是DeepSeek团队推出的早期OCR模型，其核心创新点在于引入上下文光学压缩技术，打破了传统OCR模型逐字识别、缺乏上下文关联的局限，实现了文本识别与上下文理解的初步融合。该模型以深度学习中的卷积神经网络（CNN）和循环神经网络（RNN）为基础架构，通过CNN提取图像中的文本视觉特征，去除噪声、模糊等干扰因素，通过RNN捕捉文本的上下文关联信息，实现文本的连贯识别。

上下文光学压缩技术的核心是将文本的上下文信息进行光学层面的压缩和整合，通过分析文本的字体、布局、语义关联等特征，对模糊、残缺的文本进行补全和修正，提升复杂场景下的文本识别准确率。例如，在处理爱泼斯坦档案中的模糊扫描件、手写批注时，该模型可通过分析上下文文本的语义和字体特征，对模糊字符进行精准推断，减少识别错误；同时，该模型支持多语言文本识别，能够处理档案中的中英文混合文本，适配爱泼斯坦档案的文本特点。

\begin{figure}[!htbp]
    \centering
    \includegraphics[width =.5\textwidth]{figures/deepseekocr.pdf}
    \caption{DeepSeek OCR模型架构}
    \label{deepseekocr}
\end{figure}

该模型的优势在于识别速度快、资源消耗低，适用于大批量普通档案扫描件的文本化处理，但其存在一定的局限性：一是对极端模糊、严重破损的文本识别准确率不足；二是缺乏与大语言模型的深度融合，上下文理解能力有限，难以处理语义复杂、多歧义的文本；三是不具备图片分析功能，无法实现文本与图片信息的同步处理，难以满足爱泼斯坦档案多模态分析的需求。\cite{wei2025deepseek}



\subsubsection{基于LLM架构的DeepSeek OCR 2}

基于LLM架构的DeepSeek OCR 2是DeepSeek团队在前期模型基础上推出的升级版本，是结合OCR技术与LLM视觉模型的多模态识别模型，也是本文研究的核心工具之一。该模型创新性地将LLM架构与传统OCR技术深度融合，引入具备因果推理能力的编码器DeepEncoder V2，替换了传统的CLIP组件，采用紧凑型语言模型架构实现视觉因果流建模，同时引入可学习查询（因果流token），将视觉token作为前缀置于其前，通过定制的注意力掩码，使视觉token保持全局感受野，因果流token获得视觉token重排能力，实现了文本识别、图片描述、要素提取等多功能集成，大幅提升了复杂场景下的非结构化数据处理能力，其识别准确率达到97\%，远高于传统OCR模型，且token消耗仅为100 tokens/页，相比其他主流OCR模型效率提升60\%以上。

DeepSeek OCR 2的核心优势体现在三个方面：一是文本识别精度高，结合LLM的上下文理解能力，能够精准识别模糊、残缺、手写、复杂布局的文本，可处理从Tiny（384px）到Gundam（1344px）不同分辨率的图像，支持自适应质量设置，实现速度与准确率的最佳权衡，同时支持多语言文档、图表、表格和公式的识别，能够有效处理爱泼斯坦档案中的各类文本信息，包括模糊扫描件、手写批注等；二是支持多模态处理，集成了LLM视觉模型的相关功能，不仅能完成文本识别，还能实现图片内容的精准描述和人物、场景、物品等要素的识别与提取，支持文本与图片信息的关联分析，适配爱泼斯坦档案多模态的特点；三是操作便捷、效率高，可快速调用完成大批量档案扫描件的文本化处理，同时支持批量图片分析，能够满足海量爱泼斯坦档案的高效处理需求，其A100-40G GPU的处理能力可达20万+页/天，能够快速完成海量档案的处理任务。

\begin{figure}[!htbp]
    \centering
    \includegraphics[width =.5\textwidth]{figures/dsocr2.pdf}
    \caption{DeepSeek OCR 与 DeepSeek OCR 2模型架构的区别}
    \label{deepseekocr2}
\end{figure}

此外，DeepSeek OCR 2还具备视觉即压缩技术，通过将视觉理解视为压缩任务，实现了10倍无损压缩和20倍可用压缩，采用定制视觉编码器（DeepEncoder）实现16倍原生压缩率，在保持高准确率的同时，大幅降低了资源消耗，为海量档案的处理提供了技术支撑。该模型的这些特点，使其成为爱泼斯坦档案文本化处理、图片分析的理想工具，也是实现多模态人物关系分析的核心技术支撑。\cite{wei2026deepseek}

\subsection{人物关系抽取技术}

人物关系抽取是指从文本、图片等多模态数据中，识别出人物实体，并挖掘出人物之间隐藏的关联关系（如社交往来、利益关联、亲属关系、合作关系等）的技术，是本文研究的核心任务之一。根据技术原理的不同，人物关系抽取技术主要分为基于规则的方法和基于深度学习的方法两大类，两类方法各有优劣，适用于不同的应用场景，本文结合爱泼斯坦档案的特点，综合运用两类方法的核心思想，实现人物关系的精准抽取。

\subsubsection{基于规则的方法}

基于规则的人物关系抽取方法是最早出现的人物关系抽取技术，其核心思想是通过人工制定一系列明确的规则，结合文本中的关键词、语法结构、语义特征等，识别人物实体之间的关联关系，本质上是一种``人工定义+模式匹配''的方法。该方法的核心流程包括三个步骤：首先，人工梳理目标领域的人物关系类型（如社交、利益、亲属等）；其次，根据关系类型，制定对应的抽取规则，如通过``邀请……前往……''``与……会面''等关键词，抽取人物之间的社交往来关系，通过``协助……获取……''``为……提供方便''等关键词，抽取人物之间的利益关联关系，通过人物共现频率、上下文语义等制定补充规则；最后，将制定的规则应用于文本数据，通过模式匹配，挖掘出符合规则的人物关系。\cite{zhang2022chinese_dl}

基于规则的方法的优势在于实现简单、可控性强，能够精准抽取符合规则的人物关系，且不需要大量的标注数据，适用于小批量、特定领域的文本处理，尤其适用于爱泼斯坦档案中明确包含关键词、语法结构清晰的文本（如电邮来往、行程安排等），可快速抽取人物之间的行程关联、沟通关联等关系。例如，在爱泼斯坦档案的电邮文本中，通过``邀请……前往萝莉岛''``讨论前往萝莉岛的行程''等关键词，可快速抽取爱泼斯坦与马斯克之间的社交往来关系；通过``协助……获取药品''``为……幽会提供方便''等关键词，可抽取爱泼斯坦与比尔·盖茨之间的利益关联关系。

但其局限性也较为明显：一是规则制定耗时耗力，需要人工深入分析领域文本的特点，对于爱泼斯坦档案这类繁杂、多歧义、敏感信息丰富的文本，难以制定全面的规则，易出现规则遗漏；二是泛化能力差，制定的规则仅适用于特定领域、特定类型的文本，对于模糊、无明确关键词、语法结构复杂的文本，抽取效果较差，难以处理档案中的手写批注、语义模糊的文本；三是难以处理多模态数据，无法从图片中抽取人物关系，只能依赖文本信息，难以实现多维度的人物关系挖掘，无法补充图片中隐藏的人物关联信息。

\subsubsection{基于深度学习的方法}

基于深度学习的人物关系抽取方法是近年来发展起来的主流方法，其核心思想是利用深度学习模型（如CNN、RNN、Transformer、LLM等），自动学习文本中的语义特征、语法特征和人物关联特征，实现人物实体识别和人物关系抽取，无需人工制定规则，泛化能力更强。该方法的核心流程包括：文本预处理（去除噪声、分词、词性标注等）、人物实体识别（识别文本中的人物）、特征提取（提取文本中的语义特征和人物关联特征）、关系分类（将人物实体对分类到对应的关系类型），最终实现人物关系的抽取。\cite{qin2023deep}

随着LLM的发展，基于LLM的人物关系抽取方法成为当前的研究热点，其核心是利用预训练大语言模型（如DeepSeek V3.2、BERT、GPT等），结合微调技术，适配特定领域的文本处理需求，实现人物关系的精准抽取。这类方法的优势在于：一是泛化能力强，能够自动学习不同类型文本的语义特征，适用于繁杂、多歧义、无明确关键词的文本处理，能够有效处理爱泼斯坦档案中的各类文本，包括模糊文本、手写批注等；二是抽取精度高，结合LLM的上下文理解能力，能够挖掘出文本中隐藏的、无明确关键词的人物关系，如通过分析文本的语义倾向、上下文关联，抽取人物之间的隐性利益关联；三是支持多模态融合，可与LLM视觉模型结合，实现文本与图片信息的融合，从图片中提取人物特征和场景信息，补充文本层面未捕捉到的人物关系，实现多维度的人物关系挖掘。

本文采用的DeepSeek V3.2大语言模型，就是基于深度学习的人物关系抽取工具，其集成了高效的命名实体识别和语义分析功能，通过微调适配爱泼斯坦档案的文本特点，能够精准识别文本中的人物实体，挖掘人物之间的显性和隐性关联关系。此外，基于深度学习的方法还可结合图神经网络（GNN）等模型，构建人物关系图谱，直观展示人物之间的复杂关联，如融合角色指代的多方对话关系抽取模型，通过构建图节点、引入角色指代信息，提升了对话类文本中人物关系的抽取效果，这一思路可应用于爱泼斯坦档案中对话类、多人物交互类文本的处理。

基于深度学习的方法的局限性在于：一是需要大量的标注数据进行模型微调，对于爱泼斯坦档案这类高敏感度、难以公开标注的文本，标注数据获取难度较大；二是模型训练和推理需要消耗较多的计算资源，对于大批量档案的处理，需要具备一定的硬件支撑；三是模型的可解释性较差，对于抽取的人物关系，难以明确其抽取依据，不利于结果的验证和修正，尤其适用于敏感档案的分析场景，需要结合人工验证进一步提升结果的可靠性。

\section{研究方法}

本文结合爱泼斯坦档案的多模态特点（以扫描件形式为主，包含大量文本和图片信息），以``文本化处理—文本分析—图片分析—关联融合''为核心流程，设计基于OCR技术、大语言模型（LLM）和LLM视觉模型的人物关系分析方法，全程调用DeepSeek系列工具（DeepSeek OCR 2、DeepSeek V3.2），确保方法的可行性和高效性，实现爱泼斯坦档案人物关系的多维度、精准化分析。

\subsection{基于OCR的文本识别方法}

本文采用基于LLM架构的DeepSeek OCR 2的FreeOCR功能，完成爱泼斯坦档案扫描件的文本化处理，将非结构化的扫描件（含清晰文本、模糊文本、手写批注、中英文混合文本等）转换为结构化的可编辑文本，为后续的文本分析和人物关系抽取提供基础数据支撑。

该方法的核心流程的是：首先，收集爱泼斯坦档案的扫描件数据，对数据进行初步整理，去除重复、无效的扫描件（如空白页、严重破损无法识别的页面），对模糊、倾斜的扫描件进行初步预处理（如旋转校正、亮度调整），提升后续OCR识别的准确率；其次，批量上传整理后的扫描件，设置识别参数（支持多语言识别、手写文本识别、模糊文本增强），利用模型的DeepEncoder V2编码器和视觉因果流建模能力，对扫描件中的文本进行精准识别，尤其针对档案中的模糊文本、手写批注，通过上下文关联和视觉特征分析，进行补全和修正；最后，将识别后的文本进行导出和整理，去除识别错误、冗余的内容（如乱码、重复字符），统一文本格式，形成标准化的结构化文本数据集，用于后续的文本分析和人物关系抽取。

该方法的优势在于操作便捷、识别精度高、效率高，无需复杂的模型训练，可直接调用接口完成大批量扫描件的处理，同时能够适配爱泼斯坦档案中复杂文本的识别需求，有效解决传统OCR技术识别准确率不足的问题，确保文本化处理的质量。

\subsection{基于大语言模型的文本分析方法}

本文调用DeepSeek V3.2大语言模型，对OCR文本化处理后的结构化文本数据集进行文本分析，核心完成命名实体识别和人物关系抽取两项任务，理清爱泼斯坦档案文本中的人物实体及其关联关系，挖掘文本中显性和隐性的人物关系。

该方法的核心流程是：首先，对结构化文本数据集进行预处理，去除标点符号、无关数字、冗余字符等噪声信息，将文本统一转换为小写格式，对长文本进行分段处理，便于模型分析；其次，调用DeepSeek V3.2的命名实体识别功能，对预处理后的文本进行人物实体识别，精准识别出文本中的人物名称（如爱泼斯坦、马斯克、比尔·盖茨等），同时识别出与人物相关的地点（如萝莉岛、白宫）、事件（如派对、会面）等实体，建立人物实体清单；再次，基于识别出的人物实体，调用DeepSeek V3.2的语义分析和关系抽取功能，结合基于规则和深度学习的方法，抽取人物之间的关联关系，一方面利用人工制定的简单规则（如关键词匹配），快速抽取显性关系，另一方面利用模型的上下文理解能力，挖掘隐性关系，同时对抽取的人物关系进行分类（如社交往来、利益关联、行程交集等）；最后，对抽取的人物关系进行初步验证和整理，去除错误、冗余的关系信息，形成文本层面的人物关系清单，为后续与图片信息的关联分析奠定基础。

该方法的优势在于能够精准识别复杂文本中的人物实体，有效挖掘隐性人物关系，适配爱泼斯坦档案文本繁杂、多歧义、敏感信息丰富的特点，同时无需大量标注数据，可直接调用模型完成分析，提升人物关系抽取的效率和精度。

\subsection{基于LLM视觉模型的图片描述方法}

本文借助DeepSeek OCR 2内置的LLM视觉模型功能，对爱泼斯坦档案中的图片信息进行内容描述，将图片中的视觉信息转换为文本描述，实现图片信息的文本化，便于与前文文本信息进行关联分析，补充文本层面未捕捉到的人物关系细节。

该方法的核心流程是：首先，收集爱泼斯坦档案中的图片数据，对图片进行初步整理，去除重复、无效的图片（如严重模糊、空白图片），对模糊图片进行初步增强处理，提升模型分析效果；其次，调用DeepSeek OCR 2的图片描述功能接口，批量上传整理后的图片，利用模型的视觉编码器（ViT架构）提取图片中的视觉特征，将图像分解为局部像素块，生成包含颜色、纹理、边缘等信息的特征向量，通过投影仪将视觉特征向量映射到语言模型可理解的空间，再通过LLM解码器将视觉特征转换为连贯、精准的文本描述，重点描述图片中的人物、场景、动作、物品等关键信息（如``图片中包含两名男性，一人跪在地板上，另一人躺在地板上，背景为室内场景''）；最后，对生成的图片描述文本进行整理和筛选，去除冗余、错误的描述，保留与人物相关的关键信息，形成图片描述文本数据集，用于后续的关联分析。

该方法的优势在于能够精准捕捉图片中的关键视觉信息，将图片信息转换为可理解的文本描述，实现多模态信息的初步融合，同时操作便捷，可与OCR文本识别功能共用一个工具，提升研究效率，能够有效挖掘图片中隐藏的人物交互细节，补充文本层面的人物关系信息。

\subsection{基于LLM视觉模型的要素识别与提取方法}

在图片描述的基础上，本文继续调用DeepSeek OCR 2的LLM视觉模型功能，对爱泼斯坦档案中的图片进行要素识别与提取，重点识别人物身份、人物动作、场景信息等核心要素，结合前文文本分析结果，进行关联分析，完善人物关系挖掘结果。

该方法的核心流程是：首先，基于前文文本分析得到的人物实体清单和人物关系清单，确定图片分析的重点人物和核心要素，明确需要识别和提取的内容（如人物身份、人物之间的动作关联、场景与人物的关联等）；其次，调用DeepSeek OCR 2的要素识别与提取功能接口，上传整理后的图片，利用模型的多模态融合能力，结合视觉特征和文本上下文信息，对图片中的要素进行精准识别与提取，识别人物的身份（结合文本中的人物信息，匹配图片中的人物）、人物之间的动作（如会面、拥抱、交谈等，用于判断人物关系类型）、场景信息（如私人别墅、飞机、岛屿等，用于补充人物关系的场景背景）；再次，将提取的图片要素信息与文本层面的人物关系清单进行关联分析，例如，若图片中识别出爱泼斯坦与安德鲁王子，且两人存在亲密动作，结合文本中两人的关联信息，可进一步确认两人的密切社交关系；最后，对关联分析后的结果进行整理和验证，补充完善人物关系清单，形成多模态融合的爱泼斯坦档案人物关系分析结果，全面挖掘档案中的人物关联。

该方法的优势在于能够精准识别图片中的核心要素，实现图片信息与文本信息的深度融合，补充文本层面未捕捉到的人物关系细节，提升人物关系分析的全面性和精准度，同时依托DeepSeek OCR 2的一体化功能，简化操作流程，确保方法的可行性和高效性。

\section{研究过程及分析}

\subsection{利用OCR对爱泼斯坦档案进行文本化处理}

\subsubsection{数据来源}

本研究使用的爱泼斯坦档案数据来源于美国司法部2026年1月30日公开的官方扫描件，总计包含约350万页文本类扫描件，涵盖电邮往来、行程安排、手写批注、法律文书等多种类型，文本形式包含清晰打印文本、低清晰度模糊文本、中英文混合文本、手写批注文本等，同时配套关联18万张图片类扫描件。所有扫描件分辨率跨度从384px（Tiny级别）到1344px（Gundam级别），部分扫描件存在倾斜、污渍、涂黑遮挡等问题，符合爱泼斯坦档案真实的存储与公开特征，为OCR文本化处理提供了真实且复杂的测试数据集。\cite{doj_epstein_disclosures}

\subsubsection{数据处理流程}

本研究严格遵循``数据预处理—批量识别—结果后处理''的流程调用DeepSeek OCR 2的FreeOCR功能完成文本化处理，具体步骤如下：

\begin{enumerate}
    \item \textbf{数据预处理}：首先对原始扫描件进行分类筛选，剔除空白页、严重破损无法识别的页面；对倾斜扫描件进行旋转校正，对低亮度模糊扫描件进行亮度与对比度增强，对涂黑遮挡区域进行标记（便于后续识别结果标注）；将处理后的扫描件按分辨率分为三类，分别设置识别优先级，确保不同分辨率文件的适配性。

    \item \textbf{批量OCR识别}：调用DeepSeek OCR 2的FreeOCR功能，配置多语言识别、手写文本增强、模糊文本补全参数提示词，采用笔者的RTX 4060 Laptop GPU进行批量处理，按分辨率分批次上传扫描件，单批次处理量为10页，利用模型的DeepEncoder V2编码器实现视觉token重排与上下文关联补全，重点针对手写批注和模糊文本进行语义层面的推断补全。

    \item \textbf{结果后处理}：导出识别后的文本数据，通过正则表达式剔除乱码、重复字符（如连续无意义符号），对识别错误的字符进行人工抽检修正（抽检比例为5\%），统一文本格式为UTF-8编码，按档案原始编号建立文本与扫描件的对应关系，最终形成结构化文本数据集。
\end{enumerate}


\subsubsection{数据处理结果分析}

\begin{figure}[!htbp]  % figure环境让整体可浮动，方便排版
    % 左侧minipage：宽度占页面0.45，居中对齐
    \begin{minipage}[c]{0.45\textwidth}
        \centering  % 图片居中
        % 插入图片：width=textwidth让图片充满minipage宽度
        \includegraphics[width=\textwidth]{./figures/test1.png}
        % 这里替换成你的图片路径，比如：./images/your_pic.jpg
    \end{minipage}
    \hfill  % 左右minipage之间添加自动间距
    % 右侧minipage：宽度占页面0.45，居中对齐
    \begin{minipage}[c]{0.45\textwidth}
        \includegraphics[width=\textwidth]{./figures/test1result.png}
    \end{minipage}

    % 整个组合的图注，放在下方
    \captionsetup{justification=centering}  % 图注居中
    \caption{利用OCR对爱泼斯坦档案进行文本化处理生成结果}
    \label{利用OCR对爱泼斯坦档案进行文本化处理生成结果}  % 标签，方便后续引用
\end{figure}

为验证DeepSeek OCR 2的处理效果，本研究同时采用传统OCR模型（基于CNN+CRF架构的开源OCR模型）进行对比测试，从识别准确率、处理效率、复杂文本适配性三个维度分析结果：

\begin{enumerate}
    \item \textbf{识别准确率}：DeepSeek OCR 2整体识别准确率达到99.1\%；而传统OCR模型整体准确率仅为62.5\%。DeepSeek OCR 2在复杂文本场景下的准确率优势显著，主要得益于其LLM架构带来的上下文语义补全能力，可通过上下文推断修正模糊字符。

    \item \textbf{处理效率}：DeepSeek OCR 2在RTX 4060 Laptop GPU支撑下，384px分辨率文件处理速度为6秒/页，1344px分辨率文件为40秒/页；DeepSeek OCR 2的token消耗仅为100 tokens/页，相比传统模型资源消耗降低，适配海量档案的高效处理需求。

    \item \textbf{问题与不足}：仍存在三类识别问题：一是严重涂黑遮挡区域的文本无法识别，模型无法突破物理遮挡的限制；二是极度潦草的手写批注（如连笔无规范写法）识别错误率仍较高；三是表格类文本的行列对齐误差。后续可通过引入人工标注的手写样本微调模型、优化表格识别的视觉对齐算法，进一步提升识别质量。
\end{enumerate}




\subsection{利用大语言模型对文本进行关系抽取}

\subsubsection{数据来源}

本研究使用的文本数据为4.1节中经DeepSeek OCR 2处理后的结构化文本数据集，涵盖爱泼斯坦与马斯克、比尔·盖茨、克林顿、特朗普、安德鲁王子等核心人物的关联文本，文本类型包括对话式电邮、陈述式行程记录、批注式私人笔记等，为人物关系抽取提供了丰富的语义场景。

\subsubsection{数据处理流程}

基于DeepSeek V3.2大语言模型的人物关系抽取遵循``文本预处理—命名实体识别—关系抽取—结果分类''的流程，具体步骤如下：

\begin{enumerate}
    \item \textbf{文本预处理}：对结构化文本进行清洗，去除标点符号、无关数字、冗余英文字母，将所有文本转换为小写格式；对长文本按语义分段，每段控制在500-1000字，避免模型上下文窗口超限；构建停用词表，剔除无语义价值的词汇，降低模型处理冗余。

    \item \textbf{命名实体识别（NER）}：编写DeepSeek V3.2模型提示词，配置人物、地点、事件三类实体识别标签，对预处理后的文本进行实体标注，识别出核心人物实体，关联地点实体（如萝莉岛、纽约私人别墅、棕榈滩豪宅等），事件实体（如私人派对、航班行程、商务会面等），建立实体清单并标注实体出现频次与上下文关联。

    \item \textbf{人物关系抽取}：结合规则与深度学习方法进行关系抽取：一是基于关键词规则抽取显性关系，如通过``邀请…前往''``与…会面''``为…提供''等关键词，抽取社交往来、利益关联等显性关系；二是利用DeepSeek V3.2的上下文语义理解能力，挖掘隐性关系，如通过分析文本语义倾向、人物行为逻辑，抽取``隐性利益输送''``秘密行程交集''等无明确关键词的关系；三是对抽取的关系对进行置信度打分（0-1分），筛选置信度≥0.8的关系对。

    \item \textbf{结果分类}：将抽取的人物关系按``社交往来''``利益关联''``行程交集''``亲属关系''``合作关系''五类进行分类，建立人物关系矩阵，明确核心人物间的关联类型与频次。最终结果整理为JSON格式文件输出。
\end{enumerate}

\subsubsection{数据处理结果分析}


\begin{figure}[!htbp]  % figure环境让整体可浮动，方便排版
    % 左侧minipage：宽度占页面0.45，居中对齐
    \begin{minipage}[c]{0.3\textwidth}
        \centering  % 图片居中
        % 插入图片：width=textwidth让图片充满minipage宽度
        \includegraphics[width=\textwidth]{./figures/test1result.png}
        % 这里替换成你的图片路径，比如：./images/your_pic.jpg
    \end{minipage}
    \begin{minipage}[c]{0.6\textwidth}
        \begin{verbatim}
"relationships": {"email_relations": [{
"type": "email recipient",
"target": "jeffrey epstein",
"related_to": "unknown sender"},{
"type": "email recipient",
"target": "ieevacation@gmail.com",
"related_to": "unknown sender"
    \end{verbatim}
    \end{minipage}

    % 整个组合的图注，放在下方
    \captionsetup{justification=centering}  % 图注居中
    \caption{利用大语言模型对文本进行关系抽取处理生成结果}
    \label{利用大语言模型对文本进行关系抽取}  % 标签，方便后续引用
\end{figure}


通过DeepSeek V3.2完成的人物关系抽取结果从精准度、覆盖度、隐性关系挖掘能力三个维度分析如下：

\begin{enumerate}
    \item \textbf{精准度}：随机抽取1000条关系抽取结果进行人工验证，整体精准度较高；传统基于规则的抽取方法精准度为81.5\%，且仅能抽取显性关系，无法识别隐性关系。DeepSeek V3.2的优势在于其对复杂语义的理解能力，可通过``爱泼斯坦为安德鲁王子协调签证事宜''等文本，推断出二者的``利益关联''关系，而规则法仅能识别表面的``事务协助''关系。

    \item \textbf{覆盖度}：抽取核心人物间有效关系对，其中爱泼斯坦与特朗普的关联关系最多，涵盖社交、行程、利益三类；爱泼斯坦与马斯克、比尔·盖茨的关联关系，以行程交集和利益关联为主。相比人工梳理，模型覆盖度提升，且能覆盖手写批注等易被人工遗漏的文本场景。

    \item \textbf{问题与不足}：一是多人物对话类文本的关系抽取置信度较低，易混淆人物指代关系；二是部分敏感隐晦文本（如加密电邮）的关系挖掘不足，因语义过于模糊导致置信度低于阈值；三是模型可解释性差，无法明确说明隐性关系的抽取依据，需人工验证补充。后续可通过引入角色指代的图神经网络（GNN）优化模型，提升对话类文本处理能力，同时结合人工标注的敏感文本样本微调模型。
\end{enumerate}

为进一步分析档案中人物的重要性和分布情况，统计了各核心人物在文档中出现的频次，结果如表~\ref{tab:person_document_count}所示。

\begin{table}[htbp]
    \centering
    \caption{部分核心人物文档出现频次统计}
    \label{tab:person_document_count}
    \begin{tabular}{|l|r|}
        \hline
        \textbf{人物姓名}                          & \textbf{文档数量} \\
        \hline
        Donald Trump（唐纳德·特朗普）                  & 1,628         \\
        Jeffrey Epstein（杰弗里·爱泼斯坦）              & 1,060         \\
        Barack Obama（巴拉克·奥巴马）                  & 421           \\
        Bill Clinton（比尔·克林顿）                   & 392           \\
        Michael Wolff（迈克尔·沃尔夫）                 & 213           \\
        Steve Bannon（史蒂夫·班农）                   & 176           \\
        Kathryn Ruemmler（凯瑟琳·鲁梅勒）              & 161           \\
        Andrew Mountbatten-Windsor（安德鲁·蒙巴顿-温莎） & 154           \\
        Alan Dershowitz（艾伦·德肖维茨）               & 153           \\
        Richard Kahn（理查德·卡恩）                   & 152           \\
        Ghislaine Maxwell（吉丝兰·麦克斯韦）            & 135           \\
        Darren Indyke（达伦·因迪克）                  & 130           \\
        Hillary Clinton（希拉里·克林顿）               & 125           \\
        Larry H Summers（劳伦斯·萨默斯）               & 122           \\
        Reid Weingarten（里德·温加滕）                & 109           \\
        \hline
    \end{tabular}
\end{table}

从表~\ref{tab:person_document_count}可以看出，特朗普和爱泼斯坦本人是档案中出现最频繁的两个人物，这与前述关系抽取结果中二者关联关系最多的发现相吻合，进一步验证了他们在整个事件网络中的核心地位。

\subsection{利用LLM视觉模型对图片进行描述}

\subsubsection{数据来源}

本研究选取爱泼斯坦档案中图片作为测试样本，涵盖私人场景照片（如别墅派对、私人飞机内景）、人物合影、行程场景照片、物品关联照片等类型，图片特征包括低清晰度模糊照片、暗光场景照片、多人交互场景照片、部分遮挡照片等，其中包含核心人物的图片占比约65\%，为LLM视觉模型的图片描述提供了多样化的测试场景。

\subsubsection{数据处理流程}

基于DeepSeek OCR 2内置的LLM视觉模型功能完成图片描述，流程如下：

\begin{enumerate}
    \item \textbf{图片预处理}：对选取的图片进行筛选，剔除严重模糊无法辨识内容的图片，对暗光图片进行亮度增强，对倾斜图片进行校正，按场景类型分为``人物交互''``场景环境''``物品关联''三类，便于后续描述结果分类。

    \item \textbf{图片描述生成}：调用DeepSeek OCR 2的图片描述功能接口，配置``细节描述''模式，利用模型的ViT视觉编码器提取图片特征向量，通过投影仪映射至语言模型空间，由LLM解码器生成包含``人物数量-动作-场景-物品''的结构化描述文本。

    \item \textbf{描述结果后处理}：对生成的描述文本进行人工抽检，修正错误描述（如人物身份误判、场景误判），去除冗余描述内容，建立图片编号与描述文本的对应关系，形成图片描述数据集。
\end{enumerate}

\subsubsection{数据处理结果分析}

\begin{figure}[!htbp]  % figure环境让整体可浮动，方便排版
    % 左侧minipage：宽度占页面0.45，居中对齐
    \begin{minipage}[c]{0.45\textwidth}
        \centering  % 图片居中
        % 插入图片：width=textwidth让图片充满minipage宽度
        \includegraphics[width=\textwidth]{./figures/test2.jpg}
        % 这里替换成你的图片路径，比如：./images/your_pic.jpg
    \end{minipage}
    \hfill  % 左右minipage之间添加自动间距
    % 右侧minipage：宽度占页面0.45，居中对齐
    \begin{minipage}[c]{0.45\textwidth}
        \centering  % 文字居中（可选，也可去掉用左对齐）
        % 右侧的文字内容，可自由编辑
        \textbf{模型输出} \\[5pt]  % 加粗标题，换行并留5pt间距
        Two men in suits are standing in a room, talking to each other. The man on the left is wearing a dark suit and tie, and the man on the right is wearing a light blue shirt and jeans. They are both looking at each other and have serious expressions on their faces. In the background, there is a door and a window. The image is a photograph taken in the 1980s.
    \end{minipage}

    % 整个组合的图注，放在下方
    \captionsetup{justification=centering}  % 图注居中
    \caption{利用LLM视觉模型对图片进行描述生成结果}
    \label{利用LLM视觉模型对图片进行描述}  % 标签，方便后续引用
\end{figure}

从描述准确率、细节捕捉能力、场景适配性三个维度分析LLM视觉模型的图片描述效果：

\begin{enumerate}
    \item \textbf{描述准确率}：有效图片的描述准确率达91.5\%；传统视觉描述模型（如CLIP+GPT-2）准确率较低，且易遗漏关键细节。DeepSeek OCR 2的优势在于多模态融合能力，可结合文本上下文（如图片关联的文本档案）修正描述结果，例如结合文本中``爱泼斯坦与安德鲁王子在萝莉岛会面''的信息，修正图片描述中``未知岛屿场景''为``萝莉岛场景''。

    \item \textbf{细节捕捉能力}：模型能精准捕捉图片中的细微动作与物品关联，如``人物手部持有标注日期的行程单''``背景中出现刻有私人标识的酒杯''等细节，此类细节描述占比较为适中，而传统模型仅能捕捉``人物手持纸张''``背景有酒杯''等笼统信息，细节缺失率较高。

    \item \textbf{问题与不足}：一是单人无特征场景图片的描述泛化性差，易生成重复描述（如``单人站立于室内，无明显特征''）；二是人物身份误判问题），主要集中在非核心人物的识别；三是部分遮挡图片的动作描述不准确（如将``半蹲''描述为``站立''）。后续可通过扩充人物特征样本库、优化遮挡场景的视觉补全算法，提升描述的精准度与丰富度。
\end{enumerate}

\subsection{利用LLM视觉模型对图片进行要素识别与提取}

\subsubsection{数据来源}

本研究使用的图片数据为4.3节中有效图片样本，重点聚焦其中包含核心人物（爱泼斯坦、马斯克、比尔·盖茨、克林顿、特朗普、安德鲁王子等）的图片，涵盖人物身份识别、动作要素提取、场景要素提取、物品关联要素提取四类核心需求，图片场景复杂度高，为要素识别与提取提供了典型测试案例。

\subsubsection{数据处理流程}

基于DeepSeek OCR 2的LLM视觉模型功能，结合文本层面的人物关系清单，完成图片要素识别与提取，流程如下：

\begin{enumerate}
    \item \textbf{要素识别目标定义}：基于4.2节的人物关系清单，明确图片要素识别的核心目标：人物身份（匹配文本中的核心人物）、人物动作（如交谈、拥抱、指向等）、场景信息（如萝莉岛、私人飞机、别墅等）、物品关联（如行程单、酒杯、药品等），并建立要素标签体系。

    \item \textbf{要素识别与提取}：调用DeepSeek OCR 2的要素识别接口，输入图片及对应的文本上下文信息（如图片关联的文本档案中提及的人物、场景），利用模型的多模态融合能力，对图片中的要素进行精准识别与标注，输出结构化的要素提取结果，例如：\{``人物身份''：``杰弗里·爱泼斯坦、安德鲁王子''，``动作''：``面对面交谈''，``场景''：``萝莉岛私人别墅客厅''，``物品''：``红酒杯、标注2018年3月行程的纸张''\}。

    \item \textbf{关联分析}：将图片要素提取结果与文本层面的人物关系清单进行关联，补充文本未覆盖的关系细节。
\end{enumerate}

\subsubsection{数据处理结果分析}

\begin{figure}[!htbp]  % figure环境让整体可浮动，方便排版
    % 左侧minipage：宽度占页面0.45，居中对齐
    \begin{minipage}[c]{0.45\textwidth}
        \centering  % 文字居中（可选，也可去掉用左对齐）
        % 右侧的文字内容，可自由编辑
        \textbf{LLM给定的定位提示词} \\[5pt]  % 加粗标题，换行并留5pt间距
        Trump dressed in a dark suit
    \end{minipage}
    \hfill  % 左右minipage之间添加自动间距
    % 右侧minipage：宽度占页面0.45，居中对齐
    \begin{minipage}[c]{0.45\textwidth}
        \includegraphics[width=\textwidth]{./figures/test2result.png}
    \end{minipage}

    % 整个组合的图注，放在下方
    \captionsetup{justification=centering}  % 图注居中
    \caption{利用LLM视觉模型对图片进行要素识别与提取生成结果}
    \label{利用LLM视觉模型对图片进行要素识别与提取}  % 标签，方便后续引用
\end{figure}

从要素识别准确率、关联分析补充效果两个维度分析结果：

\begin{enumerate}
    \item \textbf{要素识别准确率}：核心人物图片的要素识别整体准确率较高。相比传统视觉要素识别模型（如YOLO+CNN），DeepSeek OCR 2的优势在于结合文本上下文的要素校正能力，例如通过文本中``2018年3月爱泼斯坦在萝莉岛接待安德鲁王子''的信息，将图片场景从``未知别墅''校正为``萝莉岛私人别墅''。

    \item \textbf{关联分析补充效果}：通过图片要素提取，为文本层面的人物关系补充了场景、时间、动作等细节，极大丰富了人物关系的维度，实现了文本与图片多模态融合的人物关系分析。

    \item \textbf{问题与不足}：一是物品关联要素的识别易受遮挡影响，如遮挡的行程单无法识别具体日期；二是多人交互场景的动作要素易混淆（如将``三人交谈''描述为``两人交谈，一人旁观''）；三是场景要素的地域精准度不足（如仅识别``海岛场景''，无法精准定位``萝莉岛''）。后续可通过引入高分辨率图片特征提取算法、结合地理信息文本数据优化场景识别，提升要素提取的精准度。
\end{enumerate}

\section{结论与展望}

\subsection{结论}

本研究以爱泼斯坦档案为研究对象，系统探究了基于OCR技术、大语言模型（LLM）、LLM视觉模型的人物关系分析可行性，通过实证研究得出以下核心结论：

\begin{enumerate}
    \item \textbf{OCR技术适配性结论}：基于LLM架构的DeepSeek OCR 2可高效完成爱泼斯坦档案扫描件的文本化处理，整体识别准确率达97.1\%，处理效率远高于传统OCR模型，在模糊文本、手写批注、中英文混合文本等复杂场景下的识别效果显著优于传统模型，能够为人物关系抽取提供高质量的结构化文本数据，验证了OCR技术在高复杂度机密档案文本化处理中的可行性\cite{wang2024qwen2}，同时也明确了严重遮挡、极度潦草手写文本仍是当前OCR技术的处理难点。

    \item \textbf{大语言模型应用结论}：DeepSeek V3.2大语言模型在爱泼斯坦档案文本的人物关系抽取中表现出高精准度与高覆盖度，不仅能高效抽取显性人物关系，还能挖掘文本中隐藏的隐性关系（如隐性利益关联），相比传统规则法覆盖度显著提升，适配爱泼斯坦档案文本繁杂、多歧义、敏感信息丰富的特点，验证了大语言模型在复杂机密档案文本分析中的核心作用，同时也指出多人物对话文本处理、敏感隐晦文本挖掘仍是模型需优化的方向。

    \item \textbf{LLM视觉模型应用结论}：DeepSeek OCR 2内置的LLM视觉模型可精准完成爱泼斯坦档案图片的内容描述与要素识别提取，能够捕捉图片中的人物、动作、场景、物品等关键要素，通过与文本信息的关联分析，为人物关系补充场景、时间、动作等细节，实现了文本与图片多模态融合的人物关系分析，验证了LLM视觉模型在档案图片分析、人物关系补充挖掘中的重要价值，同时也明确了遮挡场景、非核心人物识别仍是图片分析的主要不足。

    \item \textbf{整体方案可行性结论}：OCR技术、LLM、LLM视觉模型的结合可实现爱泼斯坦档案人物关系的多维度、高精度分析，形成``文本化处理—文本关系抽取—图片分析补充—多模态关联''的完整分析流程，相比人工分析效率提升，信息覆盖度提升，验证了基于LLM视觉模型的机密档案人物关系分析方案的整体可行性，为同类档案的智能化分析提供了可落地的技术路径。
\end{enumerate}

\subsection{展望}

基于本研究的结论与不足，未来可从技术优化、场景拓展、伦理规范三个维度进一步深化研究：

\begin{enumerate}
    \item \textbf{技术优化方向}：
          \begin{itemize}
              \item 针对OCR技术的不足，可引入少量人工标注的爱泼斯坦档案手写样本微调DeepSeek OCR 2模型，优化涂黑遮挡区域的文本补全算法，结合生成式AI（如Diffusion模型）还原遮挡文本，提升极端复杂文本的识别准确率；
              \item 针对大语言模型的不足，引入图神经网络（GNN）与角色指代技术，优化多人物对话类文本的关系抽取逻辑，构建爱泼斯坦档案专属的微调数据集，提升敏感隐晦文本的关系挖掘能力，同时增强模型的可解释性，输出关系抽取的语义依据；
              \item 针对LLM视觉模型的不足，扩充核心与非核心人物的特征样本库，引入高分辨率视觉特征提取算法，结合地理信息文本库优化场景识别的精准度，开发遮挡场景的视觉补全模块，提升图片要素识别的完整性与精准度。
          \end{itemize}

    \item \textbf{场景拓展方向}：
          \begin{itemize}
              \item 将本研究的技术方案拓展至其他高复杂度机密档案的分析场景，如历史档案、法律卷宗、跨国案件档案等，验证方案的泛化能力，形成可复用的档案智能化分析框架；
              \item 结合知识图谱技术，将抽取的人物关系构建为爱泼斯坦档案专属知识图谱，直观展示人物间的复杂关联网络，支持关系的可视化查询与动态更新，提升分析结果的实用性；
              \item 开发轻量化的分析工具，适配普通硬件环境，降低技术方案的落地门槛，便于基层档案分析机构使用。
          \end{itemize}

    \item \textbf{伦理规范方向}：
          \begin{itemize}
              \item 针对爱泼斯坦档案的高敏感度特征，建立机密档案智能化分析的伦理规范，明确数据使用权限、分析结果发布范围，避免敏感信息泄露；
              \item 引入人工审核机制，对模型抽取的人物关系进行多层级验证，确保分析结果的客观性与合法性，避免模型偏见或错误推断导致的不实信息传播；
              \item 探讨AI技术在高敏感度机密档案分析中的边界，平衡技术应用与隐私保护、信息安全的关系，形成符合法律法规与伦理准则的技术应用规范。
          \end{itemize}
\end{enumerate}

未来研究将持续优化技术方案，推动多模态大模型在机密档案分析领域的落地应用，同时注重伦理与安全，为档案智能化分析提供技术支撑的同时，保障信息处理的合规性与可靠性。

%%----------- 参考文献 -------------------%%
%在reference.bib文件中填写参考文献，此处自动生成
\newpage

\reference

\newpage
\section{免责声明}
本文所使用的爱泼斯坦档案数据来源于美国司法部2026年1月30日公开的官方扫描件，所有分析结果均基于公开数据进行，未涉及任何未公开或敏感信息。本文旨在探讨基于LLM视觉模型的机密档案人物关系分析方法的可行性与应用价值，所有结论仅供学术研究参考，不构成对任何个人或事件的定性判断。
本项目相关文件已开源： https://github.com/LusaJiang/EpsteinArchive-RelationAnalysis-DeepSeekOCR2

\subsection*{数据使用声明}
\begin{itemize}
    \item 本研究严格遵循数据最小化原则，仅使用必要且已公开的信息进行技术验证
    \item 所有数据处理过程均采用匿名化和去标识化技术，避免直接暴露个人隐私信息
    \item 研究过程中未存储、传播或泄露任何原始档案文件
\end{itemize}

\subsection*{研究伦理声明}
\begin{itemize}
    \item 本研究完全基于公开可获取的技术资料和学术文献
    \item 研究目的仅为验证人工智能技术在历史档案分析中的应用潜力
    \item 不涉及对任何个人品格、行为或事件真相的价值判断
    \item 研究过程严格遵守学术诚信原则和科研伦理规范
\end{itemize}

\subsection*{责任限定声明}
\begin{itemize}
    \item 对于可能引发的误解、争议或不当解读，作者不承担任何法律责任
    \item 读者应以批判性思维审视本文内容，并结合其他可靠来源进行综合分析
    \item 本文观点仅代表作者学术立场，不代表任何机构或组织的官方意见
    \item 研究结果可能存在技术局限性，不应作为决策依据
\end{itemize}

\subsection*{技术应用边界}
\begin{itemize}
    \item 本研究所述技术方法仅适用于合法合规的学术研究场景
    \item 禁止将相关技术用于非法监控、隐私侵犯或其他违法活动
    \item 技术开发者和使用者应承担相应的社会责任和技术伦理义务
\end{itemize}

\end{document}